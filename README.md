# speechemotionrecognition

Speech Emotion Recognition, as described in this study, uses Neural Networks to classify the 
emotions expressed in a given speech (SER). It is based on the concept that voice tone and pitch 
frequently reflect underlying emotion. Speech Emotion Recognition aids in the classification of 
elicited emotions. The MLP-Classifier is a tool for classifying emotions in a circumstance. As wave 
signal, allowing for flexible learning rate selection. RAVDESS (Ryerson Audio-Visual Dataset 
Emotional Speech and Song Database dataset) will be used. Different parameters which are MFCC, 
Contrast, Mel Spectrograph Frequency, and Chroma, which are used to extract the features from 
given audio input. The dataset will be labelled using decimal encoding, which will make feature 
extraction from the audio script easier. Further testing yields an accuracy of 80.28 percent, and 
testing using an input audio sample yields the same result.
